import torch
import torch.nn as nn
from modules.size_two_dimensional import SizeTwoDimensional
from abc import abstractmethod
from modules.inside_model_gradient_clipping import InsideModelGradientClamping
from util.tensor_list_chunking import TensorListChunking
from util.tensor_utils import TensorUtils
import util.image_visualization
from data_preprocessing.last_minute_padding import LastMinutePadding

class ActivationsResizer:

    def __init__(self, network):
        self.network = network

    @abstractmethod
    def create_resized_activations(self, activations):
        raise RuntimeError("not implemented")

    @abstractmethod
    def get_number_of_output_channels(self):
        raise RuntimeError("not implemented")

    def get_real_network(self):
        real_network = (self.network.module if isinstance(self.network, torch.nn.DataParallel) else self.network)
        return real_network


class SumActivationsResizer(ActivationsResizer):

    def __init__(self, network):
        super(SumActivationsResizer, self).__init__(network)

    def create_resized_activations(self, activations):
        activations_height = activations.size(2)

        # It can be that the activations dimensionality is not exactly 1, in which case it is reduced to 1
        # by simple summation
        if activations_height != 1:
            # raise RuntimeError("Error: the height dimension of returned activations should be of size 1, but it " +
            #                   "was: " + str(activations.size(2)))
            # print("WARNING: activations height is " + str(activations_height) + " ( > 1) :\n" +
            #      "Converting to a height of 1 by summing over the rows")
            activations = torch.sum(activations, dim=2)
            # activations = activations[:, :, 0, :] # Wrong and not really faster
        return activations

    def get_number_of_output_channels(self):
        return self.network.get_number_of_output_channels()


# This activations resizer concatenates the activation rows along the
# channels dimension, rather than summing them, keeping all information
# The motivation is that summing the information of the rows collapses
# the information in a way that is most likely not meaningful, and could
# therefore frustrate effective learning. Keeping all information is safer
# and only requires the linear layer of NetworkToSoftMaxNetwork to be sized
# such that it can process the larger input
class KeepAllActivationsResizer(ActivationsResizer):

    def __init__(self, network, data_height: int):
        super(KeepAllActivationsResizer, self).__init__(network)
        self.data_height = data_height

    def create_resized_activations(self, activations):
        activations_height = activations.size(2)

        # print("create_resized_activations - activations.size: " + str(activations.size()))

        # It can be that the activations dimensionality is not exactly 1, in which case it is reduced to 1
        # by simple summation
        if activations_height != 1:
            # raise RuntimeError("Error: the height dimension of returned activations should be of size 1, but it " +
            #                   "was: " + str(activations.size(2)))
            # print("WARNING: activations height is " + str(activations_height) + " ( > 1) :\n" +
            #      "Converting to a height of 1 by summing over the rows")
            activation_rows_list = list([])
            for i in range(0, activations.size(2)):
                activation_row = activations[:, :, i, :]
                activation_rows_list.append(activation_row)
            # Concatenate the activation rows on the channel dimension
            result = torch.cat(activation_rows_list, dim=1)
        else:
            result = activations
        # print("create_resized_activations - result.size: " +str(result.size()))
        return result

    def get_number_of_output_channels(self):


        print("KeepAllActivationsResizer.get_number_of_output_channels  - data_height: " + str(self.data_height))
        print("KeepAllActivationsResizer.get_number_of_output_channels  - network.height_reduction_factor: " +
              str(self.get_real_network().get_height_reduction_factor()))
        number_of_rows_generated_by_network = int(self.data_height / self.get_real_network().get_height_reduction_factor())
        print("Number of rows generated by network: " + str(number_of_rows_generated_by_network))
        print("KeepAllActivationsResizer.get_number_of_output_channels  - network.get_number_of_output_channels(): " +
              str(self.get_real_network().get_number_of_output_channels()))
        return number_of_rows_generated_by_network * self.get_real_network().get_number_of_output_channels()


# This network takes a network as input and adds a linear layer
# that maps the input network's output to a sequential output
# of dimension: batch_size * number_of_output_channels * number_of_classes
class NetworkToSoftMaxNetwork(torch.nn.Module):
    def __init__(self, network,
                 number_of_classes_excluding_blank: int,
                 activations_resizer: ActivationsResizer,
                 clamp_gradients: bool,
                 input_is_list: bool,
                 use_block_mdlstm: bool
                 ):
        super(NetworkToSoftMaxNetwork, self).__init__()
        self.clamp_gradients = clamp_gradients
        self.input_is_list = input_is_list
        self.use_block_mdlstm = use_block_mdlstm
        self.network = network
        self.activations_resizer = activations_resizer
        self.number_of_output_channels = activations_resizer.get_number_of_output_channels()
        self.number_of_classes_excluding_blank = number_of_classes_excluding_blank

        print(">>> number_of_output_channels: " + str(self.number_of_output_channels))

        print("NetworkToSoftMaxNetwork - number of classes: " + str(self.get_number_of_classes_including_blank()))
        self.fc3 = nn.Linear(self.number_of_output_channels, self.get_number_of_classes_including_blank())



        # It is not totally clear actually whether "xavier_normal" or "xavier_uniform" initialization
        # is to be preferred
        # https://datascience.stackexchange.com/questions/13061/
        # when-to-use-he-or-glorot-normal-initialization-over-uniform-init-and-what-are
        #
        # However, the paper "Handwriting Recognition with Large Multidimensional
        # Long Short Term Memory Recurrent Neural Networks" gives better results
        # with using Xavier Glorot uniform initialization throughout, so we
        # go with that as well
        #  See: https://ieeexplore.ieee.org/document/7814068/

        # Initialize the linear output layer with Xavier uniform  weights
        # torch.nn.init.xavier_normal_(self.fc3.weight)
        torch.nn.init.xavier_uniform_(self.fc3.weight)
        # print("self.fc3 : " + str(self.fc3))
        # print("self.fc3.weight: " + str(self.fc3.weight))
        # print("self.fc3.bias: " + str(self.fc3.bias))

        print("NetworkToSoftMaxNetwork - clamp_gradients: " + str(clamp_gradients))

    @staticmethod
    def create_network_to_soft_max_network(network,
                                           number_of_classes_excluding_blank: int,
                                           data_height: int, clamp_gradients: bool,
                                           input_is_list: bool,
                                           use_block_mdlstm: bool=False):
        activations_resizer = KeepAllActivationsResizer(network, data_height)
        # activations_resizer = SumActivationsResizer(network)
        return NetworkToSoftMaxNetwork(network, number_of_classes_excluding_blank,
                                       activations_resizer,
                                       clamp_gradients, input_is_list,
                                       use_block_mdlstm
                                       )

    def get_number_of_classes_including_blank(self):
        return self.number_of_classes_excluding_blank + 1

    def set_training(self, training):
        self.get_real_network().set_training(training)

    @staticmethod
    def collect_examples_activation_heights(activations):
        examples_activation_heights = list([])
        for example_activations in activations:
            example_activations_height = example_activations.size(1)
            examples_activation_heights.append(example_activations_height)
        return examples_activation_heights

    @staticmethod
    def collect_examples_activation_widths(activations):
        examples_activation_widths = list([])
        for example_activations in activations:
            example_activations_width = example_activations.size(2)
            examples_activation_widths.append(example_activations_width)
        return examples_activation_widths

    @staticmethod
    def compute_paired_lists_multiples(int_list_one, int_list_two):
        result = list([])
        for element_one, element_two in zip(int_list_one, int_list_two):
            multiple = element_one * element_two
            # print("compute_paired_lists_multiples - element one: " +
            #       str(element_one) + " element two: "
            #       + str(element_two))
            # print("compute_paired_lists_multiples - multiple: " + str(multiple))
            result.append(multiple)
        return result


    # This debugging method checks that de-chunking the chunked tensor recovers the original
    @staticmethod
    def check_dechunking_chunked_tensor_list_recovers_original(tensor_list_chunking, original_tensor_list,
                                                       input_chunked):
        input_dechunked = tensor_list_chunking.dechunk_block_tensor_concatenated_along_batch_dimension(
            input_chunked)
        if not TensorUtils.tensors_lists_are_equal(original_tensor_list, input_dechunked):
            for index in range(0, len(original_tensor_list)):
                print("original[" + str(index) + "].size()" + str(original_tensor_list[index].size()))

            for index in range(0, len(input_dechunked)):
                print("input_dechunked[" + str(index) + "].size()" + str(input_dechunked[index].size()))

            TensorUtils.find_equal_slices_over_batch_dimension(input_chunked)

            raise RuntimeError("Error: original and de-chunked chunked are not the same")

    # This debugging method looks if it can find equal rows in the activation
    # (which in general should typically not happen)
    @ staticmethod
    def check_activation_rows_are_not_equal(activation_rows):
        # For debugging
        print("activation rows sizes after splitting: ")
        last_activation_row = activation_rows[0]
        for activation_row in activation_rows:
            print(str(activation_row.size()))
            if TensorUtils.tensors_are_equal(last_activation_row, activation_row):
                print(">>> WARNING: activation rows are equal")

    # Extract the summed rows from a chunk that consists multiple rows
    @staticmethod
    def extract_summed_rows_from_chunk_with_concatenated_rows(chunk_multiple_rows, number_of_rows, number_of_columns):
        # print("chunk multiple rows.size(): " + str(chunk_multiple_rows.size()))

        # print("number of columns: " + str(number_of_columns) +
        #      " number of rows: " + str(number_of_rows))
        # Notice the dimension to split on is 1, as for example we have
        # chunk multiple rows.size(): torch.Size([1, 14, 80])
        # That is, the last dimension goes over classes, the first one is
        # always 1, and the second dimension goes over the width.
        # Therefore we have to split on dim=1 using the number_of_columns
        # for the tensor containing the horizontally-concatenated
        # row activations
        rows = torch.split(chunk_multiple_rows, number_of_columns, dim=1)
        if len(rows) != number_of_rows:
            raise RuntimeError("Error in split: expected " + str(number_of_rows)
                               + "rows but got: " + str(len(rows)))

        summed_rows = TensorUtils.sum_list_of_tensors(rows)
        # print("summed_rows.size(): " + str(summed_rows.size()))
        return summed_rows

    @staticmethod
    def extract_activation_chunks(examples_activation_heights, examples_activation_widths,
                                  class_activations_resized_temp):
        examples_activation_height_times_width = NetworkToSoftMaxNetwork. \
            compute_paired_lists_multiples(examples_activation_heights, examples_activation_widths)
        chunks_multiple_rows = torch.split(class_activations_resized_temp,
                                           examples_activation_height_times_width, 1)
        chunks = list([])

        for index in range(0, len(chunks_multiple_rows)):
            chunk_multiple_rows = chunks_multiple_rows[index]
            number_of_rows = examples_activation_heights[index]
            number_of_columns = examples_activation_widths[index]
            summed_rows = NetworkToSoftMaxNetwork.\
                extract_summed_rows_from_chunk_with_concatenated_rows(chunk_multiple_rows, number_of_rows,
                                                                      number_of_columns)
            chunks.append(summed_rows)


        return chunks

    def compute_activations_with_block_mdlstm(self, x):
        # print("network_to_softmax_network - network input x sizes: " )
        # for element in x:
        #     print(">>> input list element size - " + str(element.size()))
        network_consumed_block_size = SizeTwoDimensional(self.get_real_network().get_height_reduction_factor(),
                                                         self.get_real_network().get_width_reduction_factor())

        # # Plot two row images for debugging
        # for element in x:
        #     if element.size(1) > 64:
        #         print("image to be plotted size: " + str(element.size()))
        #         element_without_channel_dimension = element.squeeze(0)
        #         util.image_visualization.imshow_tensor_2d(element_without_channel_dimension)

        tensor_list_chunking = TensorListChunking.create_tensor_list_chunking(x, network_consumed_block_size)

        input_chunked = tensor_list_chunking. \
            chunk_tensor_list_into_blocks_concatenate_along_batch_dimension(x, False)

        # print("input_chunked.size(): " + str(input_chunked.size()))

        # Debugging: check that the de-chunked version recovers the original
        NetworkToSoftMaxNetwork. \
            check_dechunking_chunked_tensor_list_recovers_original(tensor_list_chunking, x, input_chunked)

        # print("input_chunked :" + str(input_chunked))

        activations_chunked = self.network(input_chunked)
        # print("network_to_softmax_network - activations_chunked.size(): " + str(activations_chunked.size()))
        activations = tensor_list_chunking. \
            dechunk_block_tensor_concatenated_along_batch_dimension_changed_block_size(activations_chunked,
                                                                                       SizeTwoDimensional(1, 1))
        # print("network_to_softmax_network - activations sizes after dechunking: ")
        # for element in activations:
        #     print(">>> activations list element size - " + str(element.size()))

        activations_height_one = list([])
        for activation in activations:
            if activation.size(1) > 1:
                # print("activation.size(): " + str(activation.size()))
                activation_rows = torch.split(activation, 1, dim=1)
                # Debugging check
                # NetworkToSoftMaxNetwork.check_activation_rows_are_not_equal(activation_rows)
                activations_height_one.extend(activation_rows)
            else:
                activations_height_one.append(activation)

        # for activation in activations_height_one:
        #     print("activations_height_one_element size: " + str(activation.size()))

        # Create tensor with all activations concatenated on width dimension
        activations_single_tensor = torch.cat(activations_height_one, 2)
        activations_single_tensor = activations_single_tensor.unsqueeze(0)
        activations = activations_single_tensor

        return activations

    def forward(self, x):

        if self.input_is_list:

            if self.use_block_mdlstm:
                activations = self.compute_activations_with_block_mdlstm(x)
                examples_activation_heights = NetworkToSoftMaxNetwork.collect_examples_activation_heights(activations)
                examples_activation_widths = NetworkToSoftMaxNetwork.collect_examples_activation_widths(activations)
            else:
                last_minute_padding = LastMinutePadding(self.get_height_reduction_factor(),
                                                        self.get_width_reduction_factor())
                padded_examples_tensor = last_minute_padding.pad_and_cat_list_of_examples(x)
                activations = self.network(padded_examples_tensor)
        else:
            activations = self.network(x)

        batch_size = activations.size(0)
        # print(">>> activations.size(): " + str(activations.size()))
        # print("activations: " + str(activations))

        activations_height = activations.size(2)

        # Activations resizing should only be done in the old way with activations
        # resizer if the input is not a list
        if not self.input_is_list:
            activations = self.activations_resizer.create_resized_activations(activations)

        activations_height_removed = activations.squeeze(2)
        activations_with_swapped_channels_and_width = activations_height_removed.transpose(1, 2)
        # print(">>> activations_with_swapped_channels_and_width.size(): " +
        #      str(activations_with_swapped_channels_and_width.size()))
        activations_resized_one_dimensional = activations_with_swapped_channels_and_width.contiguous().\
            view(-1, self.number_of_output_channels)

        # print("activations_resized_one_dimensional: " + str(activations_resized_one_dimensional))
        class_activations = self.fc3(activations_resized_one_dimensional)

        if self.clamp_gradients:
            # print("NetworkToSoftMaxNetwork - register gradient clamping...")
            class_activations = InsideModelGradientClamping.register_gradient_clamping(class_activations)

        # print("class_activations: " + str(class_activations))
        if self.input_is_list and self.use_block_mdlstm:
            class_activations_resized_temp = class_activations.view(1, -1, self.get_number_of_classes_including_blank())
            # print("class_activations_resized_temp.size(): " + str(class_activations_resized_temp.size()))
            # print("examples_activation_widths: " + str(examples_activation_widths))

            chunks = NetworkToSoftMaxNetwork.extract_activation_chunks(examples_activation_heights,
                                                                       examples_activation_widths,
                                                                       class_activations_resized_temp)

            max_width = max(examples_activation_widths)
            chunks_padded = list([])
            for chunk in chunks:
                columns_padding_required = max_width - chunk.size(1)
                p1d = (0, columns_padding_required)
                # Padding is done to the last dimension but we need to padd the one-but last dimension
                # so transpose, padd, then transpose back
                chunk_transposed = chunk.transpose(1, 2)
                chunk_transposed_padded = torch.nn.functional.pad(chunk_transposed, p1d, "constant", 0)
                chunk_padded = chunk_transposed_padded.transpose(1, 2)
                chunks_padded.append(chunk_padded)
            class_activations_resized = torch.cat(chunks_padded, 0)
        else:
            class_activations_resized = class_activations.view(batch_size, -1,
                                                               self.get_number_of_classes_including_blank())



        # print("class_activations.size(): " + str(class_activations.size()))

        # print("class_activation_resized: " + str(class_activations_resized))
        # print(">>> class_activations_resized.size(): " +
        #      str(class_activations_resized.size()))

        # print(">>> MultiDimensionalRNNToSoftMaxNetwork.forward.class activations: " + str(class_activations))
        # The dimension along which softmax must make probabilities to sum to one is the classes dimension
        probabilities_sum_to_one_dimension = 2
        # result = torch.nn.functional.log_softmax(class_activations_resized, probabilities_sum_to_one_dimension)
        # result = torch.nn.functional.softmax(class_activations_resized, probabilities_sum_to_one_dimension)

        # https://github.com/SeanNaren/deepspeech.pytorch/issues/136
        # "SeanNaren:
        # warp-ctc does the softmax in the function,
        # which is why we have this inference based softmax added to the network!"
        # Don't compute softmax
        result = class_activations_resized

        # print(">>> MultiDimensionalRNNToSoftMaxNetwork.forward.result: " + str(result))
        return result

    def get_real_network(self):
        return self.activations_resizer.get_real_network()

    # Get the factor by which the original input width is reduced in the output
    # of the network
    def get_width_reduction_factor(self):
        return self.get_real_network().get_width_reduction_factor()

    # Get the factor by which the original input height is reduced in the output
    # of the network
    def get_height_reduction_factor(self):
        return self.get_real_network().get_height_reduction_factor()



